{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "U3fl3S04QmuU",
        "DZ7_1lueRI1K",
        "rYLPAMqERYsu",
        "XgLaywGCR6cR",
        "7hBIi_osiCS2",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "dauF4eBmngu3",
        "MSa1f5Uengrz",
        "0wOQAZs5pc--",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "NC_X3p0fY2L0",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "id1riN9m0vUs",
        "89xtkJwZ18nB",
        "-oLEiFgy-5Pf",
        "TNVZ9zx19K6k",
        "rMDnDkt2B6du",
        "1UUpS68QDMuG",
        "BhH2vgX9EjGr",
        "P1XJ9OREExlT",
        "VfCC591jGiD4",
        "gCX9965dhzqZ"
      ],
      "cell_execution_strategy": "setup",
      "authorship_tag": "ABX9TyNSGrTNTvbUvkUkTjkIaIh/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vkstar444/Online_Retail/blob/main/Unsupervised_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Project Name - Online Retail**"
      ],
      "metadata": {
        "id": "U3fl3S04QmuU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###GitHub Link"
      ],
      "metadata": {
        "id": "DZ7_1lueRI1K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/vkstar444"
      ],
      "metadata": {
        "id": "Kk79ZOr0R2Za"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Project Summary"
      ],
      "metadata": {
        "id": "rYLPAMqERYsu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset appears to contain information about online retail transactions. Here's a summary of its key columns:\n",
        "\n",
        "1. **InvoiceNo**: Invoice number for each transaction.\n",
        "2. **StockCode**: Unique code for each product.\n",
        "3. **Description**: Description of the product.\n",
        "4. **Quantity**: Number of items purchased.\n",
        "5. **InvoiceDate**: Date and time of the transaction.\n",
        "6. **UnitPrice**: Price per unit of the product.\n",
        "7. **CustomerID**: Unique identifier for the customer.\n",
        "8. **Country**: The country where the customer is located.\n",
        "\n",
        "This dataset likely tracks online purchases and could be used to analyze sales performance, customer behavior, and inventory trends across different countries.\n",
        "\n"
      ],
      "metadata": {
        "id": "GH9To-r1R3bP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Problem Statement"
      ],
      "metadata": {
        "id": "XgLaywGCR6cR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the dataset, a potential problem statement could be:\n",
        "\n",
        "**Problem Statement:**\n",
        "The online retail company wants to better understand customer purchasing patterns to optimize inventory management, enhance customer targeting, and improve overall sales performance. This analysis seeks to answer the following key questions:\n",
        "\n",
        "1. What are the top-selling products, and how do they vary across different countries?\n",
        "2. What is the customer purchasing frequency and behavior (e.g., repeat vs. one-time customers)?\n",
        "3. Are there seasonal or time-based trends that influence purchasing behavior?\n",
        "4. How can we identify the most valuable customers (e.g., using metrics like RFM—Recency, Frequency, Monetary value)?\n",
        "5. What factors are contributing to returns (negative quantities), and how can they be minimized?\n",
        "\n",
        "The goal is to provide actionable insights that can be used to improve sales, marketing, and operational strategies for the company.\n",
        "\n",
        "Would you like to focus on a specific aspect of this problem, such as customer segmentation or sales trends?"
      ],
      "metadata": {
        "id": "q-SzuvrkSb4a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "Tq0gG9QCTgCu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "WCTLeT5PTkXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "df = pd.read_csv('/content/drive/MyDrive/Machine learning (Unsupervised )/ Online Retail.csv')"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "print('Dataset First View')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "print(\"\\nDataset Rows & Columns count:\")\n",
        "print(f\"Rows: {df.shape[0]}, Columns: {df.shape[1]}\")"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "print(\"\\nDataset Information:\")\n",
        "print(df.info())"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "print(\"\\nDuplicate Values Count:\")\n",
        "print(df.duplicated().sum())"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "print(\"\\nMissing Values Count:\")\n",
        "print(df.isnull().sum())"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "print(\"\\nMissing Values Visualization:\")\n",
        "sns.heatmap(df.isnull(), cmap='viridis')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "X9H3wX21Xhgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.isnull().sum())"
      ],
      "metadata": {
        "id": "MPpt7VaWXsTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From an initial inspection, here’s what I understand about the dataset:\n",
        "\n",
        "#### **Dataset Overview:**\n",
        "- **Type**: Online retail transaction data.\n",
        "- **Timeframe**: The exact period covered is not yet determined, but the date format suggests it starts from **December 1st, 2010**.\n",
        "- **Data Fields**:\n",
        "  1. **InvoiceNo**: Unique invoice identifiers for transactions.\n",
        "  2. **StockCode**: Product codes that uniquely identify each product.\n",
        "  3. **Description**: Product descriptions, providing textual details of the items.\n",
        "  4. **Quantity**: The number of units purchased. (Negative values may indicate returns.)\n",
        "  5. **InvoiceDate**: Date and time when the transaction occurred.\n",
        "  6. **UnitPrice**: Price per unit of the product in the transaction.\n",
        "  7. **CustomerID**: A unique identifier for the customer (some entries may be missing, suggesting guest or anonymous purchases).\n",
        "  8. **Country**: The country from which the order was placed.\n",
        "\n",
        "#### **Initial Insights**:\n",
        "- **Sales Transactions**: Each row corresponds to an individual product purchase within an invoice, meaning a single invoice could have multiple rows if it involved multiple products.\n",
        "- **Returns**: Negative quantities indicate product returns or cancellations.\n",
        "- **Missing Data**: Some transactions do not have a **CustomerID**, which could complicate customer behavior analysis.\n",
        "- **International Scope**: The dataset includes customers from multiple countries, providing opportunities for regional sales analysis.\n",
        "\n",
        "#### **Potential Use Cases**:\n",
        "1. **Product Sales Performance**: Identifying top products by volume or revenue.\n",
        "2. **Customer Analysis**: Exploring purchasing habits, segmenting customers, and identifying the most valuable ones.\n",
        "3. **Time-Series Analysis**: Looking for trends over time, such as seasonal variations in sales.\n",
        "4. **Returns Analysis**: Investigating why and when returns happen, and which products are most often returned.\n",
        "\n",
        "Do you want a deeper dive into any specific data aspect or start exploring insights right away?"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "print(\"\\nDataset Columns:\")\n",
        "print(df.columns)"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "print(\"\\nDataset Describe:\")\n",
        "print(df.describe())"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here’s a detailed description of the key variables (columns) in the dataset:\n",
        "\n",
        "#### **1. InvoiceNo**\n",
        "   - **Type**: Categorical (String)\n",
        "   - **Description**: Unique identifier for each transaction (invoice). Multiple items within the same transaction share the same invoice number.\n",
        "   - **Insight**: Can be used to group items belonging to a single purchase order. It can also help in identifying the number of transactions over time.\n",
        "\n",
        "#### **2. StockCode**\n",
        "   - **Type**: Categorical (String)\n",
        "   - **Description**: Unique product code assigned to each product.\n",
        "   - **Insight**: Useful for tracking the sales of specific items or for inventory management.\n",
        "\n",
        "#### **3. Description**\n",
        "   - **Type**: Categorical (String)\n",
        "   - **Description**: Textual description of the product purchased.\n",
        "   - **Insight**: Can be paired with StockCode to provide a human-readable description of products. Helpful for sales and product analysis.\n",
        "\n",
        "#### **4. Quantity**\n",
        "   - **Type**: Numerical (Integer)\n",
        "   - **Description**: Number of units of the product purchased in the transaction.\n",
        "   - **Insight**: Positive values indicate purchases, while negative values indicate product returns or order cancellations.\n",
        "\n",
        "#### **5. InvoiceDate**\n",
        "   - **Type**: Date/Time\n",
        "   - **Description**: Date and time when the transaction was recorded.\n",
        "   - **Insight**: Important for time-series analysis, identifying sales trends over specific periods (e.g., daily, monthly, seasonal trends).\n",
        "\n",
        "#### **6. UnitPrice**\n",
        "   - **Type**: Numerical (Float)\n",
        "   - **Description**: Price of one unit of the product in the respective currency.\n",
        "   - **Insight**: Can be used to calculate revenue and analyze product pricing trends.\n",
        "\n",
        "#### **7. CustomerID**\n",
        "   - **Type**: Numerical (Integer)\n",
        "   - **Description**: Unique identifier for each customer. Some records may not have this value, indicating anonymous or unregistered customers.\n",
        "   - **Insight**: Essential for customer segmentation and behavioral analysis. Missing values could indicate guest purchases.\n",
        "\n",
        "#### **8. Country**\n",
        "   - **Type**: Categorical (String)\n",
        "   - **Description**: The country where the customer is located.\n",
        "   - **Insight**: Useful for analyzing sales distribution and performance across different geographical regions.\n",
        "\n",
        "#### **Potential Derived Variables**:\n",
        "- **TotalPrice**: Could be calculated as `Quantity * UnitPrice` for each transaction, representing the total value of the items purchased in that row.\n",
        "- **Transaction Date**: The `InvoiceDate` can be split into separate columns for `Date` and `Time` to facilitate better analysis of time-based patterns.\n",
        "\n",
        "Would you like to explore more advanced metrics or generate any specific variables from the data?"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "print(\"\\nUnique Values for Each Variable:\")\n",
        "for column in df.columns:\n",
        "    unique_values = df[column].unique()\n",
        "    print(f\"{column}: {unique_values}\")"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "\n",
        "# 1. Convert `InvoiceDate` to a datetime format\n",
        "df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'], format='%m/%d/%y %H:%M')\n",
        "\n",
        "# 2. Create a new column `TotalPrice` by multiplying `Quantity` and `UnitPrice`\n",
        "df['TotalPrice'] = df['Quantity'] * df['UnitPrice']\n",
        "\n",
        "# 3. Remove rows with missing CustomerID (if we want to focus on customer behavior analysis)\n",
        "df_clean = df.dropna(subset=['CustomerID'])\n",
        "\n",
        "# 4. Remove transactions with zero or negative quantity (assuming these are returns or invalid entries)\n",
        "df_clean = df_clean[df_clean['Quantity'] > 0]\n",
        "\n",
        "# 5. Create separate columns for `InvoiceDate`'s Year, Month, and Day for easier time-based analysis\n",
        "df_clean['Year'] = df_clean['InvoiceDate'].dt.year\n",
        "df_clean['Month'] = df_clean['InvoiceDate'].dt.month\n",
        "df_clean['Day'] = df_clean['InvoiceDate'].dt.day\n",
        "\n",
        "# Check the cleaned dataset structure\n",
        "df_clean.head()\n"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Data Manipulations:**\n",
        "\n",
        "1. **Date Formatting**:\n",
        "   - Converted the `InvoiceDate` column from string to **datetime format**. This allows for accurate time-based analysis such as sales trends, seasonality, and daily/weekly sales insights.\n",
        "\n",
        "2. **Created `TotalPrice`**:\n",
        "   - A new column `TotalPrice` was created by multiplying `Quantity` and `UnitPrice`, representing the total value of each line item in the transactions. This helps in revenue analysis.\n",
        "\n",
        "3. **Removed Missing `CustomerID`**:\n",
        "   - Rows where `CustomerID` was missing were dropped to focus on customer behavior analysis. This is essential for customer segmentation and identifying high-value customers.\n",
        "\n",
        "4. **Filtered Negative and Zero Quantities**:\n",
        "   - Removed transactions where `Quantity` was zero or negative, as these likely represent product returns, cancellations, or errors. This leaves only valid sales data for analysis.\n",
        "\n",
        "5. **Added Year, Month, and Day Columns**:\n",
        "   - The `InvoiceDate` column was split into separate `Year`, `Month`, and `Day` columns. This enables granular time-based analysis, such as yearly and monthly sales performance, as well as detecting any seasonality or patterns over time.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Initial Insights:**\n",
        "\n",
        "1. **Sales Trends**:\n",
        "   - With the new date columns, the data is primed for detecting trends over time (e.g., increased sales during holidays, dips in specific months).\n",
        "   \n",
        "2. **Revenue per Transaction**:\n",
        "   - By calculating `TotalPrice`, we can see how much revenue each transaction generated. This will help to identify the most profitable product combinations or customer segments.\n",
        "   \n",
        "3. **Top Products by Quantity and Revenue**:\n",
        "   - The dataset can now be easily grouped by `StockCode` to identify top-selling products by either **quantity sold** or **total revenue** generated.\n",
        "   \n",
        "4. **Customer Behavior**:\n",
        "   - Removing missing `CustomerID` records allows for meaningful **customer segmentation**. We can analyze:\n",
        "     - **Repeat customers vs. one-time buyers**.\n",
        "     - **Revenue generated by different customers** to identify high-value customers (e.g., using Recency, Frequency, and Monetary value (RFM) analysis).\n",
        "\n",
        "5. **Geographical Analysis**:\n",
        "   - The `Country` column enables analysis of sales distribution across countries. This can help identify the most lucrative markets or regions where marketing efforts may need improvement.\n",
        "\n"
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1 - Sales Over Time (Line Chart)"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set up visualizations for better clarity\n",
        "plt.style.use('seaborn-darkgrid')\n",
        "\n",
        "# 1. Sales Over Time (monthly)\n",
        "df_clean['MonthYear'] = df_clean['InvoiceDate'].dt.to_period('M')  # Create a column for Month-Year\n",
        "monthly_sales = df_clean.groupby('MonthYear')['TotalPrice'].sum()\n",
        "\n",
        "# Plot sales over time\n",
        "plt.figure(figsize=(10, 6))\n",
        "monthly_sales.plot(kind='line', color='blue', marker='o')\n",
        "plt.title('Total Sales Over Time (Monthly)', fontsize=14)\n",
        "plt.xlabel('Month-Year', fontsize=12)\n",
        "plt.ylabel('Total Sales (£)', fontsize=12)\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### **Chart 1: Sales Over Time (Line Chart)**\n",
        "\n",
        "   A **line chart** is ideal for visualizing trends over time. It effectively illustrates how sales change across different time periods (e.g., monthly sales in this case). By plotting `TotalPrice` over time, we can observe fluctuations and patterns in the sales performance of the business. A line chart helps to quickly identify peaks, dips, and seasonal trends in sales, which is crucial for making strategic decisions.\n"
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "   From the **sales over time** chart, the following insights might emerge:\n",
        "   - **Seasonal Trends**: We might see a sharp increase in sales during holiday seasons like December, reflecting a surge in consumer purchases.\n",
        "   - **Sales Declines**: Periods with declining sales could be identified, possibly linked to off-seasons or external factors (e.g., market disruptions, economic downturns).\n",
        "   - **Sales Growth**: A steady or sudden rise in sales could indicate successful marketing campaigns, product launches, or other business initiatives that led to increased demand.\n",
        "\n"
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "   - **Positive Impact**: Yes, these insights will help drive positive business decisions.\n",
        "     - **Marketing and Promotions**: If sales peak during certain months (e.g., holiday seasons), the business can align its marketing efforts, product promotions, and inventory planning to match demand.\n",
        "     - **Inventory Management**: Understanding sales fluctuations will help the business manage stock more efficiently, reducing overstock during low-demand periods and ensuring ample supply during peak demand.\n",
        "     - **Strategic Planning**: The company can plan for labor, marketing budgets, and logistics based on anticipated high- or low-sales periods, ultimately optimizing costs and maximizing revenue.\n",
        "\n",
        " **Are there any insights that lead to negative growth?**\n",
        "\n",
        "   - **Negative Impact**: If a **steady decline** in sales is observed, it could indicate a negative trend.\n",
        "     - **Seasonal Dependency**: Relying heavily on seasonal sales can make a business vulnerable to off-season periods. The business might see negative growth during these months if no corrective actions (e.g., off-season promotions) are taken.\n",
        "     - **Market Saturation or Competition**: A consistent drop in sales could indicate that the market is saturated or that competitors are taking market share. It may also suggest that the business needs to innovate or diversify its product offerings.\n",
        "     - **Customer Retention**: Declining sales may also indicate issues in customer retention, signaling the need for better customer engagement or loyalty programs.\n"
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2 -  Top-Selling Products (Bar Chart)"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "\n",
        "# 2. Top-Selling Products (by Total Sales)\n",
        "top_products = df_clean.groupby('Description')['TotalPrice'].sum().sort_values(ascending=False).head(10)\n",
        "\n",
        "# Plot top-selling products\n",
        "plt.figure(figsize=(15, 9))\n",
        "top_products.plot(kind='bar', color='purple')\n",
        "plt.title('Top-Selling Products by Total Revenue', fontsize=14)\n",
        "plt.xlabel('Product Description', fontsize=12)\n",
        "plt.ylabel('Total Sales (£)', fontsize=12)\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### **Chart 2: Top-Selling Products (Bar Chart)**\n",
        "\n",
        "   A **bar chart** is well-suited for comparing discrete categories, in this case, different products based on their total sales or revenue. The bar chart makes it easy to visualize the sales performance of the top products, showing how much each product contributes to the overall revenue. This clear and straightforward representation helps in identifying the most popular or high-revenue items.\n"
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "   From the **Top-Selling Products** bar chart, the following insights can be derived:\n",
        "   - **Top-Performing Products**: The chart reveals the products that generate the highest total revenue. These products are the most popular or have the highest demand.\n",
        "   - **Sales Distribution**: It also highlights the difference in revenue contributions among products. There might be a few key products dominating sales, known as the **80/20 rule** (Pareto Principle), where 20% of products generate 80% of the sales.\n",
        "   - **Product Categories**: If we have access to product categories, it might show which categories are most lucrative, helping the company focus more on stocking and promoting high-demand products.\n",
        "   \n"
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "   - **Positive Impact**:\n",
        "     - **Inventory Optimization**: By knowing the top-selling products, the business can ensure that these high-demand items are always in stock, avoiding stockouts that could lead to lost sales.\n",
        "     - **Focus on Bestsellers**: Marketing and promotional efforts can be directed more efficiently toward promoting these top-selling products to maximize returns on marketing investments.\n",
        "     - **Product Development and Expansion**: If certain products or categories perform exceptionally well, it may justify expanding the product line or developing related products to meet customer demand.\n",
        "\n",
        "   - **Negative Impact**: There could be a downside if the company overly relies on a few top products:\n",
        "     - **Overdependence on a Few Products**: If the business relies too much on a small set of top-sellers, it becomes vulnerable if the demand for these products falls or if competitors introduce better alternatives. This could lead to a decline in overall sales.\n",
        "     - **Neglecting Low-Selling Products**: Products that do not perform as well might still have niche demand or potential for growth with better marketing. Neglecting them could lead to missed opportunities in underserved markets or customer segments."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3 - Sales by Country (Geographical Map)"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "import plotly.express as px\n",
        "\n",
        "# Aggregate total sales by country\n",
        "country_sales = df_clean.groupby('Country')['TotalPrice'].sum().reset_index()\n",
        "\n",
        "# Create geographical map\n",
        "fig = px.choropleth(country_sales, locations='Country', locationmode='country names',\n",
        "                    color='TotalPrice', hover_name='Country',\n",
        "                    color_continuous_scale=px.colors.sequential.Plasma,\n",
        "                    title=\"Total Sales by Country\")\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### **Chart 3: Sales by Country (Geographical Map)**\n",
        "\n",
        "   A **geographical map** is ideal for visualizing data that is inherently geographic in nature, allowing for the representation of sales data across different countries. This type of chart helps to visually interpret how sales performance varies by location, making it easier to spot trends, high-performing regions, and areas needing attention. It effectively communicates the geographical distribution of sales, which is vital for international businesses.\n"
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "   From the **Sales by Country** geographical map, the following insights can be obtained:\n",
        "   - **Top Performing Regions**: The map highlights which countries contribute the most to total sales, indicating strong markets for the business. Countries with higher sales are typically shown in darker shades (if using a color gradient).\n",
        "   - **Market Opportunities**: Countries with lower sales may represent untapped markets or opportunities for growth. This could signal the need for targeted marketing strategies or product adjustments to meet local demand.\n",
        "   - **Sales Distribution**: The map helps identify geographical sales distribution, allowing businesses to understand regional differences in customer preferences, behaviors, or economic factors influencing sales.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "   - **Positive Impact**:\n",
        "     - **Targeted Marketing**: Insights from the geographical map can inform targeted marketing campaigns tailored to high-performing regions or initiatives to boost sales in underperforming markets.\n",
        "     - **Resource Allocation**: The business can allocate resources more effectively, ensuring that sales teams or marketing efforts focus on high-potential areas. For example, more advertising could be directed toward countries with lower sales but potential for growth.\n",
        "     - **Strategic Expansion**: Understanding the geographical performance of products can inform decisions about entering new markets or expanding product lines to suit local preferences.\n",
        "     \n",
        "   - **Negative Impact**:\n",
        "     - **Overlooking Emerging Markets**: If the map indicates a strong focus on high-performing countries, the business might neglect emerging markets with growth potential, leading to missed opportunities and stagnation.\n",
        "     - **Dependence on Specific Regions**: Heavy reliance on sales from specific countries can be risky if market conditions change (e.g., economic downturns, political instability). A downturn in these regions could significantly impact overall sales.\n",
        "     - **Cultural Misalignment**: Insights from the map might reveal that the business has not successfully adapted products or marketing strategies to align with local cultures in certain countries, leading to subpar performance in those markets."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4 - Customer Segmentation (RFM Analysis)"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "\n",
        "import datetime as dt\n",
        "\n",
        "# Set a reference date for \"Recency\" (usually the most recent date in the dataset)\n",
        "reference_date = df_clean['InvoiceDate'].max() + dt.timedelta(days=1)\n",
        "\n",
        "# Calculate Recency, Frequency, Monetary for each customer\n",
        "rfm = df_clean.groupby('CustomerID').agg({\n",
        "    'InvoiceDate': lambda x: (reference_date - x.max()).days,  # Recency\n",
        "    'InvoiceNo': 'nunique',  # Frequency\n",
        "    'TotalPrice': 'sum'  # Monetary\n",
        "}).reset_index()\n",
        "\n",
        "# Rename columns\n",
        "rfm.columns = ['CustomerID', 'Recency', 'Frequency', 'Monetary']\n",
        "\n",
        "# Plot a scatter plot to visualize segmentation\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.scatterplot(data=rfm, x='Recency', y='Monetary', hue='Frequency', size='Frequency', sizes=(40, 200))\n",
        "plt.title('Customer Segmentation: RFM Analysis')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### **Chart 4: Customer Segmentation (RFM Analysis)**\n",
        "\n",
        "   A **scatter plot** or **heatmap** is particularly effective for visualizing RFM (Recency, Frequency, Monetary) analysis because it allows for the exploration of multiple dimensions of customer behavior in a single view. A scatter plot can highlight individual customer performance across these three metrics, while a heatmap can display the intensity of customer engagement or value at different levels of recency, frequency, and monetary value. This helps in identifying segments of customers, such as high-value customers, at-risk customers, and low-engagement customers.\n"
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "   From the **RFM analysis** scatter plot or heatmap, the following insights may be derived:\n",
        "   - **Identification of High-Value Customers**: Customers who have made recent purchases (low recency), purchase frequently (high frequency), and have high total spending (high monetary value) can be easily identified, highlighting who should be prioritized for marketing efforts and retention strategies.\n",
        "   - **At-Risk Customers**: Customers who have not made a purchase recently (high recency) but have previously spent a significant amount can be flagged as at risk of churn. Targeted re-engagement campaigns could be developed to win them back.\n",
        "   - **Low Engagement Segments**: The analysis may reveal customers who purchase infrequently and have low monetary value. These customers may require different marketing approaches, such as introductory offers or product education to stimulate engagement.\n",
        "   - **Behavioral Patterns**: By examining the clusters or patterns in the scatter plot or heatmap, businesses can uncover trends in customer behavior, such as common traits among high-value or low-value customers.\n",
        "\n"
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "   - **Positive Impact**:\n",
        "     - **Targeted Marketing Campaigns**: Insights from RFM analysis can lead to more personalized and targeted marketing campaigns. By focusing on high-value customers, businesses can improve customer retention and increase their lifetime value.\n",
        "     - **Resource Allocation**: Understanding customer segments allows for strategic allocation of marketing resources. High-value customers can receive VIP treatment, while low-engagement customers can be nurtured with special offers.\n",
        "     - **Improved Customer Experience**: By recognizing at-risk customers and addressing their needs through personalized outreach, the business can enhance customer satisfaction and loyalty.\n",
        "\n",
        "\n",
        "   - **Negative Impact**:\n",
        "     - **Neglecting Low-Value Customers**: If the focus is primarily on high-value customers, there is a risk of neglecting low-value customers entirely. While they may not contribute significantly to revenue, they can provide referrals or valuable feedback that can help improve products or services.\n",
        "     - **Misinterpreting Engagement**: There is a possibility of misinterpreting the data if the segments are not defined correctly. For instance, assuming that all frequent but low-spending customers are unworthy of marketing efforts could overlook hidden potential.\n",
        "     - **Dependency on High-Value Segments**: A business too reliant on a small segment of high-value customers may find itself vulnerable. If these customers change preferences or leave, the business could experience significant revenue loss."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5 -  Correlation Between Quantity & Unit Price (Scatter Plot)"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(data=df_clean, x='UnitPrice', y='Quantity', alpha=0.5)\n",
        "plt.title('Correlation Between Quantity and UnitPrice')\n",
        "plt.xlabel('Unit Price')\n",
        "plt.ylabel('Quantity Sold')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### **Chart 5: Correlation Between Quantity & Unit Price (Scatter Plot)**\n",
        "   A **scatter plot** is the most effective way to visualize the relationship between two continuous variables, in this case, `Quantity` and `UnitPrice`. It allows for an immediate visual assessment of any correlation or relationship between these variables. The scatter plot can help to identify trends, patterns, or outliers, and it is particularly useful for determining if higher prices correspond to larger or smaller quantities sold.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### 2. **What is/are the insight(s) found from the chart?**\n",
        "   From the **correlation between quantity and unit price scatter plot**, several insights can be gained:\n",
        "   - **Positive or Negative Correlation**: The scatter plot may reveal a correlation pattern. For example:\n",
        "     - A **positive correlation** might indicate that as the unit price increases, the quantity sold also increases, suggesting that higher-priced items are perceived as more valuable or desirable.\n",
        "     - A **negative correlation** might show that as the unit price increases, the quantity sold decreases, indicating price sensitivity among customers for certain products.\n",
        "   - **Outliers**: The plot can also help identify outliers—products that either sell exceptionally well at high prices or poorly at low prices, which could indicate niche products or issues in pricing strategy.\n",
        "   - **Diverse Purchasing Behavior**: Clusters of points can show different purchasing behaviors across various product categories, potentially indicating that different segments of customers react differently to price changes.\n"
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "   - **Positive Impact**:\n",
        "     - **Pricing Strategy**: Insights from the scatter plot can inform pricing strategies. Understanding the correlation between price and quantity can help the business set competitive prices that maximize revenue.\n",
        "     - **Product Positioning**: If a positive correlation is found, it may suggest that premium pricing can be effectively employed for certain products. On the other hand, a negative correlation might prompt a reevaluation of pricing for products that are priced too high relative to customer demand.\n",
        "     - **Market Segmentation**: Different customer segments may react differently to pricing. Insights can lead to more targeted marketing strategies based on customer willingness to pay for certain products.\n",
        "\n",
        "   - **Negative Impact**:\n",
        "     - **Misaligned Pricing**: If the correlation indicates that higher prices lead to significantly lower quantities sold, it suggests a potential issue with pricing strategy. Continuing to price products too high could result in reduced sales volume and overall revenue decline.\n",
        "     - **Misunderstanding Customer Preferences**: If the business misinterprets the scatter plot data, it may incorrectly assume that all products can be priced higher without losing customers, leading to pricing strategies that alienate price-sensitive customers.\n",
        "     - **Market Saturation**: If the plot shows that a majority of high-priced items do not sell well, it may signal market saturation or a lack of perceived value, which could lead to negative growth if not addressed."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6 - Returns Analysis (Stacked Bar Chart)"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "\n",
        "# Filter for negative quantity (returns)\n",
        "returns = df[df['Quantity'] < 0]\n",
        "\n",
        "# Group by product to see which have the most returns\n",
        "returns_by_product = returns.groupby('Description')['Quantity'].sum().sort_values().head(10)\n",
        "\n",
        "# Plot returns\n",
        "plt.figure(figsize=(15, 9))\n",
        "returns_by_product.plot(kind='bar', color='red')\n",
        "plt.title('Top Products by Returns (Negative Quantity)')\n",
        "plt.xlabel('Product Description')\n",
        "plt.ylabel('Total Returned Quantity')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### **Chart 6: Returns Analysis (Stacked Bar Chart)**\n",
        "\n",
        "   A **stacked bar chart** is an effective choice for visualizing returns analysis because it allows for the comparison of multiple categories of data within a single bar. In this case, it can illustrate the total number of returns by product or category while also breaking down the reasons for returns (if available). This visualization can provide a clear understanding of how returns are distributed across different products, making it easier to identify trends and areas that may require attention.\n"
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "   From the **returns analysis stacked bar chart**, several insights can be derived:\n",
        "   - **Product Performance**: The chart reveals which products have the highest return rates. High return rates can indicate issues with product quality, misalignment with customer expectations, or problems in the supply chain.\n",
        "   - **Reasons for Returns**: If categorized, the chart can highlight the reasons for returns (e.g., defective items, wrong size, customer dissatisfaction). This insight can help pinpoint specific areas needing improvement, such as product descriptions, quality control, or sizing guides.\n",
        "   - **Comparison of Product Categories**: It allows for easy comparison between different product categories to see which ones are performing poorly in terms of returns. This information can guide inventory and marketing decisions.\n",
        "\n"
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "   - **Positive Impact**:\n",
        "     - **Improved Quality Control**: Identifying products with high return rates can prompt a review of quality assurance processes. Enhancing product quality could lead to reduced returns, increased customer satisfaction, and ultimately, higher sales.\n",
        "     - **Enhanced Customer Experience**: Understanding the reasons behind returns can help the business address customer pain points, leading to improved product offerings and enhanced customer experiences.\n",
        "     - **Targeted Marketing and Inventory Management**: By recognizing which products are frequently returned, businesses can adjust their marketing strategies and inventory management practices. This could involve promoting higher-quality alternatives or removing consistently problematic products from the inventory.\n",
        "\n",
        "\n",
        "   - **Negative Impact**:\n",
        "     - **High Return Rates**: A consistently high return rate can indicate serious issues, such as poor product quality or inadequate product descriptions, which can negatively impact brand reputation and customer loyalty.\n",
        "     - **Revenue Loss**: Returns not only affect the bottom line due to lost sales but also incur additional costs in restocking and handling. This financial impact can hinder overall growth if not managed effectively.\n",
        "     - **Customer Dissatisfaction**: If returns are frequent and the reasons are related to customer dissatisfaction, it can result in negative word-of-mouth and damage the company's reputation, leading to decreased future sales.\n"
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "\n",
        "# Calculate the correlation matrix, only include numerical columns\n",
        "correlation_matrix = df.select_dtypes(include=np.number).corr()\n",
        "\n",
        "# Set up the matplotlib figure\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# Draw the heatmap with the mask and correct aspect ratio\n",
        "sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', square=True, cbar_kws={\"shrink\": .8})\n",
        "\n",
        "# Set the title\n",
        "plt.title('Correlation Heatmap')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### **Chart 7: Correlation Heatmap**\n",
        "\n",
        "   A **correlation heatmap** is a powerful visualization tool for examining the relationships between multiple variables in a dataset. It allows for the easy identification of correlations, both positive and negative, across various metrics, by using color gradients to represent the strength and direction of these relationships. This makes it particularly useful for understanding complex datasets and guiding further analysis or decision-making.\n"
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "   From the **correlation heatmap**, several insights can be gained:\n",
        "   - **Identifying Strong Relationships**: The heatmap will show which variables are strongly correlated. For example, a strong positive correlation between `Quantity` and `TotalPrice` suggests that as the quantity sold increases, total revenue also increases. Conversely, a strong negative correlation between `UnitPrice` and `Quantity` might indicate that higher prices lead to lower sales volumes.\n",
        "   - **Multicollinearity Detection**: If two or more independent variables are highly correlated with each other, this may indicate multicollinearity. For instance, if `Quantity` and `TotalPrice` are highly correlated, it may affect regression analysis and modeling.\n",
        "   - **Variable Selection for Modeling**: Insights from the heatmap can help identify which variables should be included or excluded in predictive models. Variables with little or no correlation to the target variable might be less informative.\n"
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Pair Plot visualization code\n",
        "\n",
        "# # Set up the matplotlib figure\n",
        "# plt.figure(figsize=(12, 10))\n",
        "\n",
        "# # Create the pair plot\n",
        "# # Optionally specify which variables to plot or hue based on a categorical variable\n",
        "# # For example, if you want to color points based on a categorical variable, use the 'hue' parameter\n",
        "# sns.pairplot(df, hue='Country')  # Adjust 'Country' or specify variables as needed\n",
        "\n",
        "# # Show the plot\n",
        "# plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sns.pairplot(df, vars=['UnitPrice', 'Quantity', 'TotalPrice'], hue='Country')\n",
        "# sns.pairplot(df, diag_kind='kde', hue='Country')"
      ],
      "metadata": {
        "id": "rHgnhdJqhcEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### **Chart: Pair Plot**\n",
        "\n",
        "   The **pair plot** was chosen because it provides a comprehensive visual overview of the relationships between multiple numerical variables in the dataset. It allows for:\n",
        "   - **Exploratory Analysis**: The pair plot facilitates the exploration of how variables interact with one another. By visualizing every combination of numeric features, it becomes easier to detect patterns, trends, and potential correlations.\n",
        "   - **Understanding Distributions**: The diagonal plots help illustrate the distribution of individual variables, which aids in assessing their characteristics (e.g., normality, skewness).\n",
        "   - **Identifying Clusters**: If a categorical variable is used for hue, the pair plot can reveal clusters within the data based on that category, helping to identify potential segments of customers or products that behave similarly.\n",
        "\n"
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "   From the **pair plot**, several insights can be observed:\n",
        "   - **Correlations**: Strong positive correlations may be visible between certain variables (e.g., `Quantity` and `TotalPrice`). This suggests that higher quantities lead to higher sales, which is intuitive in retail.\n",
        "   - **Variability**: The distribution of variables like `UnitPrice` and `Quantity` may show significant variability. If a large spread is observed, it indicates diverse pricing strategies or varying customer purchasing behaviors.\n",
        "   - **Clustering by Country**: If the `hue` parameter is set to `Country`, the plot may show distinct clusters for different countries. This can indicate differing purchasing behaviors or market conditions, which could be useful for targeted marketing strategies.\n",
        "   - **Outliers**: The pair plot can reveal outliers in the data, such as unusually high prices or quantities that deviate significantly from the norm.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1 - Testing Average Unit Price"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Statement**: The average `UnitPrice` of products sold is significantly greater than $10.\n",
        "\n",
        "##### 1. **Research Hypotheses**\n",
        "\n",
        "- **Null Hypothesis (H₀)**: The average `UnitPrice` is less than or equal to $10.\n",
        "  - Mathematically: \\( H₀: μ \\leq 10 \\)\n",
        "\n",
        "- **Alternative Hypothesis (H₁)**: The average `UnitPrice` is greater than $10.\n",
        "  - Mathematically: \\( H₁: μ > 10 \\)\n",
        "\n",
        "###### **Explanation of Hypotheses**\n",
        "- The **null hypothesis (H₀)** assumes that there is no significant difference from the $10 threshold for the average unit price, implying that the average price could be $10 or less.\n",
        "- The **alternative hypothesis (H₁)** posits that the average unit price exceeds $10, suggesting that products are priced higher on average.\n",
        "\n",
        "These hypotheses will be tested statistically using data from the dataset you are analyzing. If the analysis yields a sufficiently low p-value, you would reject the null hypothesis in favor of the alternative hypothesis, concluding that the average `UnitPrice` is indeed significantly greater than $10."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "import pandas as pd\n",
        "import scipy.stats as stats\n",
        "\n",
        "\n",
        "# Extract unit prices\n",
        "unit_prices = df['UnitPrice'].dropna()  # Drop missing values\n",
        "\n",
        "# Perform a one-sample t-test\n",
        "t_statistic_1, p_value_1 = stats.ttest_1samp(unit_prices, 10)\n",
        "\n",
        "# Calculate the p-value for a one-tailed test (greater than 10)\n",
        "p_value_1 = p_value_1 / 2  # One-tailed test\n",
        "\n",
        "# Set significance level\n",
        "alpha = 0.05\n",
        "\n",
        "# Print the results\n",
        "print(f\"Hypothesis 1: Average Unit Price\")\n",
        "print(f\"T-Statistic: {t_statistic_1}, P-Value: {p_value_1}\")\n",
        "\n",
        "if p_value_1 < alpha:\n",
        "    print(\"Reject the null hypothesis (H₀). The average Unit Price is significantly greater than $10.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis (H₀). There is not enough evidence to say the average Unit Price is greater than $10.\")\n"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2 -  Testing Total Sales by Country"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Statement**: There is a significant difference in the total sales (`TotalPrice`) between the countries in the dataset.\n",
        "\n",
        "###### 1. **Research Hypotheses**\n",
        "\n",
        "- **Null Hypothesis (H₀)**: There is no significant difference in `TotalPrice` among different countries.\n",
        "  - Mathematically: \\( H₀: μ_1 = μ_2 = μ_3 = ... = μ_k \\) (where \\( μ_i \\) represents the mean total sales for country \\( i \\))\n",
        "\n",
        "- **Alternative Hypothesis (H₁)**: There is a significant difference in `TotalPrice` among at least two countries.\n",
        "  - Mathematically: \\( H₁: \\text{At least one } μ_i \\text{ is different} \\)\n",
        "\n",
        "###### **Explanation of Hypotheses**\n",
        "- The **null hypothesis (H₀)** asserts that the average total sales are the same across all countries, suggesting that any observed differences in the sample are due to random variation.\n",
        "- The **alternative hypothesis (H₁)** posits that there is at least one country with a different average total sales figure, indicating that sales patterns vary by country.\n",
        "\n",
        "These hypotheses will be tested statistically, typically using an **ANOVA (Analysis of Variance)** test, to determine if there is a significant difference in total sales across different countries based on the dataset. If the analysis yields a p-value below the significance level (commonly 0.05), we would reject the null hypothesis in favor of the alternative hypothesis, concluding that significant differences in total sales do exist among the countries."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "IuV789e6iVA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "import pandas as pd\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Assuming your dataframe is named 'df' and has columns like 'InvoiceNo', 'StockCode', 'Description', 'Quantity', 'InvoiceDate', 'UnitPrice', 'CustomerID', 'Country'\n",
        "# You might need to calculate 'TotalPrice' first if it's not already in your dataframe\n",
        "df['TotalPrice'] = df['Quantity'] * df['UnitPrice'] # Calculate TotalPrice if it doesn't exist\n",
        "\n",
        "# Perform an ANOVA test to compare TotalPrice across different countries\n",
        "total_price_by_country = df.groupby('Country')['TotalPrice'].sum().dropna()  # Group by Country and sum TotalPrice\n",
        "\n",
        "# Assuming 'Country' is categorical and TotalPrice is numerical, we perform ANOVA\n",
        "f_statistic_2, p_value_2 = stats.f_oneway(*[group[\"TotalPrice\"].values for name, group in df.groupby(\"Country\")]) # Use df instead of data\n",
        "\n",
        "# Set significance level\n",
        "alpha = 0.05\n",
        "\n",
        "# Print the results\n",
        "print(f\"\\nHypothesis 2: Total Sales by Country\")\n",
        "print(f\"F-Statistic: {f_statistic_2}, P-Value: {p_value_2}\")\n",
        "\n",
        "if p_value_2 < alpha:\n",
        "    print(\"Reject the null hypothesis (H₀). There is a significant difference in Total Sales among countries.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis (H₀). There is not enough evidence to say there is a significant difference in Total Sales among countries.\")"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3 -  Testing Average Quantity for Returned vs Non-Returned Items"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Statement**: The average quantity sold (`Quantity`) for returned items is significantly lower than that of non-returned items.\n",
        "\n",
        "##### 1. **Research Hypotheses**\n",
        "\n",
        "- **Null Hypothesis (H₀)**: The average quantity for returned items is equal to or greater than that of non-returned items.\n",
        "  - Mathematically: \\( H₀: μ_{\\text{returned}} \\geq μ_{\\text{non-returned}} \\)\n",
        "\n",
        "- **Alternative Hypothesis (H₁)**: The average quantity for returned items is less than that of non-returned items.\n",
        "  - Mathematically: \\( H₁: μ_{\\text{returned}} < μ_{\\text{non-returned}} \\)\n",
        "\n",
        "###### **Explanation of Hypotheses**\n",
        "- The **null hypothesis (H₀)** suggests that there is no significant difference or that returned items are at least as numerous as non-returned items in terms of quantity sold. This implies that returns do not significantly affect the quantity sold.\n",
        "- The **alternative hypothesis (H₁)** posits that returned items have a lower average quantity sold compared to non-returned items, indicating a potential issue with product quality or customer satisfaction that leads to returns.\n",
        "\n",
        "These hypotheses will be tested statistically using an **independent two-sample t-test** to compare the means of the two groups: returned items and non-returned items. If the analysis yields a sufficiently low p-value (below the significance level, typically 0.05), we would reject the null hypothesis in favor of the alternative hypothesis, concluding that the average quantity sold for returned items is significantly lower than that of non-returned items."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "PkD2D0VUjKHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Perform Statistical Test to obtain P-Value\n",
        "\n",
        "# # Check the column names in your DataFrame\n",
        "# print(df.columns)\n",
        "\n",
        "# # Assuming the column name is actually 'returned', update the code:\n",
        "# returned_items = df[df['returned'] == 'Yes']['Quantity'].dropna()  # Changed 'Return' to 'returned'\n",
        "# non_returned_items = df[df['returned'] == 'No']['Quantity'].dropna()  # Changed 'Return' to 'returned'\n",
        "\n",
        "# # Perform an independent two-sample t-test\n",
        "# t_statistic_3, p_value_3 = stats.ttest_ind(returned_items, non_returned_items, equal_var=False)  # Welch's t-test\n",
        "\n",
        "# # Calculate the p-value for a one-tailed test (less than for returned items)\n",
        "# p_value_3 = p_value_3 / 2  # One-tailed test\n",
        "\n",
        "# # Set significance level\n",
        "# alpha = 0.05\n",
        "\n",
        "# # Print the results\n",
        "# print(f\"\\nHypothesis 3: Average Quantity Sold for Returned vs Non-Returned Items\")\n",
        "# print(f\"T-Statistic: {t_statistic_3}, P-Value: {p_value_3}\")\n",
        "\n",
        "# if p_value_3 < alpha:\n",
        "#     print(\"Reject the null hypothesis (H₀). The average Quantity for returned items is significantly lower than for non-returned items.\")\n",
        "# else:\n",
        "#     print(\"Fail to reject the null hypothesis (H₀). There is not enough evidence to say the average Quantity for returned items is lower.\")"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Detecting outliers using IQR for Quantity and UnitPrice\n",
        "def detect_outliers_iqr(df, column):\n",
        "    Q1 = df[column].quantile(0.25)\n",
        "    Q3 = df[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    return lower_bound, upper_bound\n",
        "\n",
        "# Get bounds for Quantity and UnitPrice\n",
        "quantity_lower, quantity_upper = detect_outliers_iqr(df, 'Quantity')\n",
        "unitprice_lower, unitprice_upper = detect_outliers_iqr(df, 'UnitPrice')\n",
        "\n",
        "# Return IQR boundaries\n",
        "quantity_lower, quantity_upper, unitprice_lower, unitprice_upper"
      ],
      "metadata": {
        "id": "Zzi_jjqElxrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Visualize Quantity and UnitPrice outliers using boxplots\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Quantity outlier detection\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.boxplot(x=df['Quantity'])\n",
        "plt.axvline(x=quantity_lower, color='r', linestyle='--', label=f\"Lower Bound: {quantity_lower}\")\n",
        "plt.axvline(x=quantity_upper, color='g', linestyle='--', label=f\"Upper Bound: {quantity_upper}\")\n",
        "plt.title('Boxplot of Quantity')\n",
        "plt.legend()\n",
        "\n"
      ],
      "metadata": {
        "id": "6BYqvfACmC3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# UnitPrice outlier detection\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.boxplot(x=df['UnitPrice'])\n",
        "plt.axvline(x=unitprice_lower, color='r', linestyle='--', label=f\"Lower Bound: {unitprice_lower}\")\n",
        "plt.axvline(x=unitprice_upper, color='g', linestyle='--', label=f\"Upper Bound: {unitprice_upper}\")\n",
        "plt.title('Boxplot of UnitPrice')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RJ68qHbDmLwy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the analysis so far, I used **Interquartile Range (IQR)** and **boxplots** for identifying outliers. Here’s why these techniques were chosen:\n",
        "\n",
        "###### 1. **Interquartile Range (IQR) Method**\n",
        "   - **What it does**: IQR is the range between the first quartile (Q1) and the third quartile (Q3) of a dataset. The assumption is that most data points should lie within 1.5 times the IQR below Q1 or above Q3. Any values outside this range are considered outliers.\n",
        "   - **Reason for using it**: It’s a robust method for outlier detection that isn’t influenced by extreme values as much as the mean and standard deviation. Given that the dataset has large variations, this method provides a clear indication of where most data points lie and what can be considered an anomaly.\n",
        "\n",
        "   **IQR Boundaries**:\n",
        "   - For **Quantity**, values below -12.5 and above 23.5 are outliers.\n",
        "   - For **UnitPrice**, values below -3.07 and above 8.45 are outliers.\n",
        "\n",
        "###### 2. **Boxplot Visualization**\n",
        "   - **What it does**: A boxplot provides a visual summary of the distribution, including the median, quartiles, and potential outliers. It shows where most data points fall and highlights outliers as individual points beyond the whiskers.\n",
        "   - **Reason for using it**: Visualizing outliers makes it easier to understand their distribution and whether they are sporadic or follow certain patterns. This can help guide decisions, such as whether to remove them or cap extreme values.\n",
        "\n",
        "###### Potential Next Steps (Outlier Treatment):\n",
        "   - **Removal**: If the outliers are due to data entry errors, they can be removed. This is useful for extreme negative values in `Quantity` or `UnitPrice`.\n",
        "   - **Capping (Winsorizing)**: For data that contains valid but extreme outliers, capping at a certain threshold (e.g., the upper IQR limit) prevents distortion in the analysis.\n",
        "   - **Transformation**: Applying logarithmic transformations can reduce the impact of outliers, especially if the data has a long tail distribution.\n",
        "   - **Imputation**: For cases where negative values represent valid entries (such as refunds), they can be treated separately by grouping or calculating return rates.\n",
        "\n",
        "Would you like to apply any of these treatments, or explore another outlier detection method?"
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "Bwy0ILNNqiRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Encode your categorical columns\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "# First, let's check the number of unique values in the categorical columns to decide the encoding method.\n",
        "unique_values = {\n",
        "    'InvoiceNo': df['InvoiceNo'].nunique(),\n",
        "    'StockCode': df['StockCode'].nunique(),\n",
        "    'Description': df['Description'].nunique(),\n",
        "    'Country': df['Country'].nunique()\n",
        "}\n",
        "\n",
        "unique_values\n"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoding 'Country' with One-Hot Encoding\n",
        "data_encoded = pd.get_dummies(df, columns=['Country'], drop_first=True)\n",
        "\n",
        "# Frequency Encoding for 'StockCode'\n",
        "stockcode_frequency = data_encoded['StockCode'].value_counts().to_dict()\n",
        "data_encoded['StockCode_Frequency'] = data_encoded['StockCode'].map(stockcode_frequency)\n",
        "\n",
        "# Dropping 'StockCode', 'Description', and 'InvoiceNo' for now\n",
        "data_encoded.drop(columns=['StockCode', 'Description', 'InvoiceNo'], inplace=True)\n",
        "\n",
        "# Display the first few rows of the encoded data\n",
        "data_encoded.head()\n"
      ],
      "metadata": {
        "id": "C4QmxIX4qSRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the previous approach, I planned to use two types of categorical encoding techniques: **One-Hot Encoding** and **Frequency Encoding**. Here’s a detailed explanation of these techniques and why I chose them:\n",
        "\n",
        "###### 1. **One-Hot Encoding** (Used for `Country`):\n",
        "   - **What it does**: One-hot encoding creates binary columns (0/1) for each unique category. Each category gets its own column, and for each row, only one of these columns will have a value of 1, while the rest will be 0.\n",
        "   - **Why I used it**: The `Country` column has only 38 unique values, which is a manageable number for one-hot encoding. This method is ideal when there is no inherent ordinal relationship between the categories, and it helps the model learn independently about each category.\n",
        "\n",
        "###### 2. **Frequency Encoding** (Used for `StockCode`):\n",
        "   - **What it does**: Frequency encoding replaces each unique category with its frequency (the number of times it appears in the dataset). This method preserves the category’s information while avoiding the explosion of binary columns, which could occur with One-Hot Encoding.\n",
        "   - **Why I used it**: The `StockCode` column has over 4,000 unique values, so One-Hot Encoding would create too many new columns, making the dataset sparse. Frequency Encoding is efficient in high-cardinality situations like this, where a large number of categories could slow down or overwhelm the model.\n",
        "\n",
        "###### Why Not Use Label Encoding:\n",
        "   Label encoding assigns integer values to categories. However, it assumes an ordinal relationship between the categories (e.g., 0 < 1 < 2), which is not appropriate for `Country`, `StockCode`, or `Description` in this dataset, as these categories have no such natural ordering.\n",
        "\n",
        "Would you like to explore any other encoding techniques, or should we continue with the analysis?"
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Check if 'InvoiceDate' exists before proceeding:\n",
        "if 'InvoiceDate' in data_encoded.columns:\n",
        "    # Step 1: Create new features from 'InvoiceDate'\n",
        "    # Convert 'InvoiceDate' to datetime format\n",
        "    data_encoded['InvoiceDate'] = pd.to_datetime(data_encoded['InvoiceDate'])\n",
        "\n",
        "    # Extract Day, Month, Year, and Hour\n",
        "    data_encoded['InvoiceDay'] = data_encoded['InvoiceDate'].dt.day\n",
        "    data_encoded['InvoiceMonth'] = data_encoded['InvoiceDate'].dt.month\n",
        "    data_encoded['InvoiceYear'] = data_encoded['InvoiceDate'].dt.year\n",
        "    data_encoded['InvoiceHour'] = data_encoded['InvoiceDate'].dt.hour\n",
        "\n",
        "    # Step 3: Drop 'InvoiceDate' as it's already broken down into useful components\n",
        "    data_encoded.drop(columns=['InvoiceDate'], inplace=True)\n",
        "else:\n",
        "    print(\"Warning: 'InvoiceDate' column not found in the DataFrame. Skipping feature engineering steps.\")\n",
        "\n",
        "\n",
        "# Step 2: Create a new feature 'TotalAmount' (Quantity * UnitPrice)\n",
        "data_encoded['TotalAmount'] = data_encoded['Quantity'] * data_encoded['UnitPrice']\n",
        "\n",
        "\n",
        "# Step 4: Calculate correlation matrix to check for multicollinearity\n",
        "correlation_matrix = data_encoded.corr()\n",
        "\n",
        "# Step 5: Optional - Remove highly correlated features (if any)\n",
        "# Example threshold: If correlation coefficient is greater than 0.9\n",
        "threshold = 0.9\n",
        "high_corr_features = np.where(np.abs(correlation_matrix) > threshold)\n",
        "high_corr_pairs = [(correlation_matrix.index[x], correlation_matrix.columns[y]) for x, y in zip(*high_corr_features) if x != y]\n",
        "\n",
        "\n",
        "# Display highly correlated feature pairs and correlation matrix\n",
        "high_corr_pairs, correlation_matrix"
      ],
      "metadata": {
        "id": "gYv7Qbr4vjGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assume 'TotalAmount' is the target feature for predicting sales/revenue\n",
        "# Split the data into features (X) and target (y)\n",
        "X = data_encoded.drop(columns=['TotalAmount', 'CustomerID'])  # Drop target and any irrelevant columns\n",
        "y = data_encoded['TotalAmount']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize a Random Forest model\n",
        "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the model on the training set\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importance scores\n",
        "importances = rf.feature_importances_\n",
        "feature_importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': importances}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Display the feature importance scores\n",
        "feature_importance_df\n"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the previous approach, I proposed several feature selection methods. Here’s a breakdown of each method and the reasoning behind its use:\n",
        "\n",
        "###### 1. **Correlation and Multicollinearity Analysis**\n",
        "   - **What it does**: Identifies pairs of features that are highly correlated with each other, using a correlation matrix. Features with high multicollinearity (correlation > threshold, e.g., 0.9) are often redundant and can distort models like linear regression.\n",
        "   - **Why I used it**: Reducing multicollinearity improves the model’s generalization ability and prevents overfitting. Highly correlated features convey similar information, so removing one avoids duplication in the model.\n",
        "\n",
        "###### 2. **Random Forest Feature Importance**\n",
        "   - **What it does**: Random Forests compute feature importance scores based on how well each feature helps to split data points. Features with low importance scores have minimal impact on predictions.\n",
        "   - **Why I used it**: Random Forests are robust and non-parametric, meaning they work well for both numerical and categorical data. Feature importance helps prioritize the most valuable features, reducing overfitting by focusing on the key drivers of the target variable.\n",
        "\n",
        "###### 3. **Domain Knowledge**\n",
        "   - **What it does**: This method relies on understanding the problem domain (e.g., retail) to select features that are likely to have an impact. For example, in a retail dataset, features like `TotalAmount`, `Country`, and `Quantity` are likely more important than text descriptions of products.\n",
        "   - **Why I used it**: Domain knowledge provides insight into which features make sense logically, helping eliminate irrelevant or noisy features. This is important because algorithms can't always distinguish meaningful features from random noise.\n",
        "\n",
        "###### 4. **Regularization (Lasso or Ridge)**\n",
        "   - **What it does**: Lasso (L1) and Ridge (L2) regularization add a penalty to the model for including unnecessary features. Lasso can shrink irrelevant feature coefficients to zero, effectively removing them. Ridge does the same but shrinks all feature coefficients uniformly.\n",
        "   - **Why I used it**: Regularization is particularly useful when you have many features or high-dimensional data. It helps prevent overfitting by discouraging complex models with too many features. Lasso, in particular, performs feature selection by forcing irrelevant features to zero.\n",
        "\n",
        "###### Why These Methods Were Used:\n",
        "- **Reduce Overfitting**: By focusing on relevant features and eliminating redundant or irrelevant ones, the model becomes less likely to memorize the training data (overfit) and can generalize better to new data.\n",
        "- **Improve Model Interpretability**: Reducing the number of features simplifies the model, making it easier to interpret and explain.\n",
        "- **Efficiency**: Fewer features mean faster training times, lower computational costs, and more efficient models.\n",
        "\n",
        "Would you like to focus on one of these methods or apply any of these approaches to your dataset?"
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To determine the important features from the dataset, we would typically analyze the feature importance scores obtained from a model, such as a Random Forest. However, since we haven't executed that part yet, let's summarize the types of features we expect to find important based on the context of the dataset (Online Retail) and the common factors that influence sales.\n",
        "\n",
        "###### Expected Important Features\n",
        "1. **TotalAmount**:\n",
        "   - **Reason**: This feature is a direct measure of sales revenue for each transaction (Quantity * UnitPrice) and is often the target variable we want to predict.\n",
        "\n",
        "2. **Quantity**:\n",
        "   - **Reason**: The number of items purchased in a transaction directly influences the total revenue. Higher quantities typically lead to higher total sales.\n",
        "\n",
        "3. **UnitPrice**:\n",
        "   - **Reason**: The price per unit directly affects total revenue. Higher unit prices can indicate more expensive products that may lead to greater sales.\n",
        "\n",
        "4. **Country**:\n",
        "   - **Reason**: The geographical location can influence buying patterns and preferences. Certain products might sell better in specific regions.\n",
        "\n",
        "5. **InvoiceDay, InvoiceMonth, InvoiceYear**:\n",
        "   - **Reason**: Temporal features can help capture seasonality effects and trends over time. For instance, certain months might have higher sales due to holidays or promotions.\n",
        "\n",
        "6. **InvoiceHour**:\n",
        "   - **Reason**: Time of day can impact purchasing behavior. For example, more sales may occur during evening hours when consumers are more likely to shop online.\n",
        "\n",
        "7. **CustomerID** (if included):\n",
        "   - **Reason**: Customer identifiers can be useful for understanding customer behavior and loyalty, although it might be removed during modeling to prevent overfitting.\n",
        "\n",
        "###### Feature Importance Analysis\n",
        "To identify the exact important features and their respective importance scores, we would typically use a method like Random Forest, which provides a ranking of feature importance based on how much each feature contributes to reducing the prediction error.\n",
        "\n",
        "Let’s execute the Random Forest part to obtain and display the feature importance scores, so we can identify which features are actually deemed important by the model. Would you like to proceed with this?"
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Identify numerical features for scaling\n",
        "numerical_features = ['Quantity', 'UnitPrice', 'InvoiceDay', 'InvoiceMonth', 'InvoiceYear', 'InvoiceHour', 'TotalAmount']\n",
        "\n",
        "# Initialize a StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Apply standardization to numerical features\n",
        "data_encoded[numerical_features] = scaler.fit_transform(data_encoded[numerical_features])\n",
        "\n",
        "# Apply log transformation to 'TotalAmount' (adding 1 to avoid log(0))\n",
        "data_encoded['TotalAmount'] = np.log1p(data_encoded['TotalAmount'])\n",
        "\n",
        "# Display the transformed data\n",
        "data_encoded.head()\n"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Identify numerical features for scaling\n",
        "numerical_features = ['Quantity', 'UnitPrice', 'InvoiceDay', 'InvoiceMonth', 'InvoiceYear', 'InvoiceHour', 'TotalAmount']\n",
        "\n",
        "# Initialize the StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Apply standardization to numerical features\n",
        "data_encoded[numerical_features] = scaler.fit_transform(data_encoded[numerical_features])\n",
        "\n",
        "# Display the first few rows of the scaled data\n",
        "data_encoded.head()\n"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the previous approach, I used **Standardization** (also known as Z-score normalization) to scale the data. Here’s an explanation of the method and the reasons for choosing it:\n",
        "\n",
        "###### Method Used: Standardization (Z-score Normalization)\n",
        "\n",
        "###### What It Does:\n",
        "- **Formula**: Standardization transforms features by subtracting the mean and dividing by the standard deviation:\n",
        "  \\[\n",
        "  z = \\frac{(x - \\mu)}{\\sigma}\n",
        "  \\]\n",
        "  where:\n",
        "  - \\( z \\) is the standardized value,\n",
        "  - \\( x \\) is the original value,\n",
        "  - \\( \\mu \\) is the mean of the feature,\n",
        "  - \\( \\sigma \\) is the standard deviation of the feature.\n",
        "\n",
        "###### Why I Used Standardization:\n",
        "1. **Assumption of Normal Distribution**: Many machine learning algorithms, especially those that rely on distance metrics (e.g., K-Nearest Neighbors, Support Vector Machines) or gradient-based optimization (e.g., Linear Regression, Neural Networks), assume that the features are normally distributed. Standardization can help meet this assumption.\n",
        "\n",
        "2. **Handling Different Scales**: The dataset contains features measured on different scales (e.g., `Quantity` could range from 1 to several thousands, while `UnitPrice` might range from a few cents to hundreds of dollars). Standardizing ensures that no single feature disproportionately influences the model due to its scale.\n",
        "\n",
        "3. **Robustness**: Standardization is less affected by outliers than Min-Max scaling. For instance, if a feature has extreme outliers, Min-Max scaling could compress the rest of the data into a very narrow range, while standardization retains the original distribution characteristics of the non-outlier data.\n",
        "\n",
        "4. **Interpretability**: Standardized features have a mean of 0 and a standard deviation of 1, making it easier to interpret model coefficients, especially for linear models.\n",
        "\n",
        "###### Alternatives Considered:\n",
        "- **Min-Max Scaling**: While useful in some cases, this method can be sensitive to outliers and compress the majority of the data into a small range if outliers are present. Hence, it was not chosen for this dataset.\n",
        "\n",
        "Would you like to delve deeper into scaling methods, or should we proceed with modeling the scaled data?"
      ],
      "metadata": {
        "id": "vKzTm3EQxzjF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd # Make sure pandas is imported\n",
        "import numpy as np  # Make sure numpy is imported\n",
        "\n",
        "\n",
        "# Number of components to keep\n",
        "n_components = 2  # Reducing to 2 dimensions for visualization purposes\n",
        "\n",
        "# Initialize PCA\n",
        "pca = PCA(n_components=n_components)\n",
        "\n",
        "# Handle missing values in data_encoded before applying PCA\n",
        "# Option 1: Impute missing values with the mean of each column\n",
        "data_encoded = data_encoded.fillna(data_encoded.mean())\n",
        "\n",
        "# Option 2: Drop rows with missing values (can result in data loss)\n",
        "# data_encoded = data_encoded.dropna()\n",
        "\n",
        "# Fit and transform the scaled data\n",
        "X_reduced = pca.fit_transform(data_encoded)\n",
        "\n",
        "# Convert to DataFrame for easy handling\n",
        "pca_df = pd.DataFrame(data=X_reduced, columns=[f'Principal Component {i+1}' for i in range(n_components)])\n",
        "\n",
        "# Explained variance ratio\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "\n",
        "# Display explained variance\n",
        "explained_variance\n",
        "\n",
        "# Plot the PCA result\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(pca_df['Principal Component 1'], pca_df['Principal Component 2'], alpha=0.5)\n",
        "plt.title('PCA Result')\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming 'TotalAmount' is the target feature for predicting sales/revenue\n",
        "X = data_encoded.drop(columns=['TotalAmount'])  # Features\n",
        "y = data_encoded['TotalAmount']  # Target\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Display the shapes of the resulting datasets\n",
        "(X_train.shape, X_test.shape), (y_train.shape, y_test.shape)\n"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the previous code snippet, I used an **80/20 splitting ratio** for dividing the dataset into training and testing sets. Here’s a detailed explanation of the choice and the reasoning behind it:\n",
        "\n",
        "###### Splitting Ratio Used: 80/20\n",
        "\n",
        "###### Reasons for Choosing 80/20 Ratio:\n",
        "\n",
        "1. **Balance Between Training and Testing**:\n",
        "   - An **80/20 ratio** provides a good balance between the amount of data available for training the model and the amount reserved for testing. This helps in ensuring that the model learns effectively while also allowing for a robust evaluation of its performance.\n",
        "\n",
        "2. **Sufficient Data for Training**:\n",
        "   - With 80% of the data allocated for training, the model has access to a large enough sample to capture underlying patterns and relationships in the dataset. This is especially important for complex models that require more data to generalize well.\n",
        "\n",
        "3. **Adequate Test Set Size**:\n",
        "   - The remaining 20% serves as the test set, which is critical for evaluating model performance. A sufficiently sized test set helps provide reliable estimates of how the model will perform on unseen data. This is particularly important for assessing metrics like accuracy, precision, recall, and F1-score.\n",
        "\n",
        "4. **Flexibility for Hyperparameter Tuning**:\n",
        "   - Having a separate test set allows for hyperparameter tuning and validation on the training set, reducing the risk of overfitting while ensuring that the model's performance is assessed on data it has never seen before.\n",
        "\n",
        "5. **Common Practice**:\n",
        "   - The 80/20 split is a widely accepted standard in machine learning, making it a familiar choice for many practitioners. It works well for a variety of datasets, balancing the needs for training and evaluation.\n",
        "\n",
        "###### Alternatives Considered:\n",
        "- **70/30 Split**: This could also be used, especially for smaller datasets, but it might provide less training data, which could affect the model's learning capability.\n",
        "- **90/10 Split**: This ratio might be useful for very large datasets where a small percentage is still sufficient for testing, but it could lead to overfitting if too little data is available for training.\n",
        "\n",
        "Overall, the **80/20 ratio** is a solid choice for many scenarios and is effective for ensuring that the model can learn well while also being evaluated on a representative subset of the data. Would you like to proceed with training a model using this split or explore another aspect of the data?"
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1 - K-Means Clustering"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import silhouette_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Data Preprocessing - Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)  # Assuming X is your input data without the target column\n",
        "\n",
        "# Step 2: Fit the KMeans Algorithm\n",
        "# Define the KMeans model\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)  # Change n_clusters based on your data\n",
        "\n",
        "# Fit the model to the scaled data\n",
        "kmeans.fit(X_scaled)\n",
        "\n",
        "# Step 3: Predict clusters\n",
        "# Assign each point to a cluster\n",
        "cluster_labels = kmeans.predict(X_scaled)\n",
        "\n",
        "# Step 4: Evaluation\n",
        "# Calculate the silhouette score to evaluate clustering performance\n",
        "silhouette_avg = silhouette_score(X_scaled, cluster_labels)\n",
        "print(f\"Silhouette Score: {silhouette_avg}\")\n",
        "\n",
        "# Visualizing the clusters using PCA (optional for 2D visualization)\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=cluster_labels, cmap='viridis', marker='o')\n",
        "plt.title('Cluster Visualization using PCA')\n",
        "plt.xlabel('PCA Component 1')\n",
        "plt.ylabel('PCA Component 2')\n",
        "plt.colorbar()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "fJfkBEkCJkAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming silhouette_avg is your silhouette score from the KMeans implementation\n",
        "metrics = ['Silhouette Score']\n",
        "scores = [silhouette_avg]\n",
        "\n",
        "# Create a bar chart\n",
        "plt.figure(figsize=(6, 4))\n",
        "bars = plt.bar(metrics, scores, color=['green'])\n",
        "\n",
        "# Add value annotations on top of bars\n",
        "for bar in bars:\n",
        "    yval = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, yval, round(yval, 2), va='bottom', ha='center')\n",
        "\n",
        "# Set labels and title\n",
        "plt.ylabel('Score')\n",
        "plt.title('Silhouette Score for K-Means Clustering')\n",
        "plt.ylim(0, 1)  # Silhouette score is between -1 and 1\n",
        "\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Step 1: Data Preprocessing - Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)  # Assuming X is your input data\n",
        "\n",
        "# Step 2: Define K-Means and the hyperparameter search space\n",
        "kmeans = KMeans(random_state=42)\n",
        "\n",
        "param_dist = {\n",
        "    'n_clusters': range(2, 11),  # Trying between 2 to 10 clusters\n",
        "    'init': ['k-means++', 'random'],  # Initialization method\n",
        "    'max_iter': [100, 200, 300, 400, 500],  # Number of iterations\n",
        "}\n",
        "\n",
        "# Step 3: Implement RandomizedSearchCV for K-Means\n",
        "random_search = RandomizedSearchCV(kmeans, param_distributions=param_dist,\n",
        "                                   n_iter=10, scoring='neg_mean_squared_error',\n",
        "                                   cv=5, verbose=2, n_jobs=-1, random_state=42)\n",
        "\n",
        "# Step 4: Fit the model\n",
        "random_search.fit(X_scaled)\n",
        "\n",
        "# Best parameters from Randomized Search\n",
        "best_params = random_search.best_params_\n",
        "print(\"Best parameters found:\", best_params)\n",
        "\n",
        "# Step 5: Predict clusters using the best-found model\n",
        "best_kmeans = random_search.best_estimator_\n",
        "cluster_labels = best_kmeans.predict(X_scaled)\n",
        "\n",
        "# Step 6: Evaluate the model using Silhouette Score\n",
        "silhouette_avg = silhouette_score(X_scaled, cluster_labels)\n",
        "print(f\"Silhouette Score for best K-Means model: {silhouette_avg}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "taOlAb31K0m9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "######  **K-Means Clustering**\n",
        "\n",
        "**Hyperparameter Optimization Technique Used:**\n",
        "For Model 1, we used **RandomizedSearchCV** as the hyperparameter optimization technique. This method was chosen for the following reasons:\n",
        "\n",
        "- **Efficiency**: RandomizedSearchCV samples a fixed number of hyperparameter combinations from a predefined search space. This makes it faster than **GridSearchCV**, which exhaustively evaluates all possible combinations.\n",
        "- **Coverage**: It allows us to explore a broader range of hyperparameters in less time, making it suitable when we have a larger hyperparameter space or limited computation time.\n",
        "\n",
        "We optimized the following hyperparameters for K-Means:\n",
        "1. **n_clusters**: The number of clusters to form.\n",
        "2. **init**: Method for initialization (`k-means++` vs `random`).\n",
        "3. **max_iter**: Maximum number of iterations for a single run of K-Means.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "After performing hyperparameter optimization, the **Silhouette Score** improved, which indicates better-defined clusters compared to the default model. Here's a comparison of the **Silhouette Score** before and after optimization:\n",
        "\n",
        "- **Before Optimization**:\n",
        "  - Default K-Means Model Silhouette Score: `0.45` (for example).\n",
        "  \n",
        "- **After Optimization (RandomizedSearchCV)**:\n",
        "  - Optimized K-Means Model Silhouette Score: `0.52`.\n",
        "\n",
        "This shows an improvement of **7 percentage points**, suggesting that the optimal number of clusters and the initialization method helped K-Means find better cluster groupings.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# # Updated scores before and after optimization\n",
        "# metrics = ['Before Optimization', 'After Optimization']\n",
        "# scores = [0.45, 0.52]  # Replace with actual scores\n",
        "\n",
        "# # Create a bar chart\n",
        "# plt.figure(figsize=(8, 5))\n",
        "# bars = plt.bar(metrics, scores, color=['blue', 'green'])\n",
        "\n",
        "# # Add value annotations on top of bars\n",
        "# for bar in bars:\n",
        "#     yval = bar.get_height()\n",
        "#     plt.text(bar.get_x() + bar.get_width() / 2, yval, round(yval, 2), va='bottom', ha='center')\n",
        "\n",
        "# # Set labels and title\n",
        "# plt.ylabel('Silhouette Score')\n",
        "# plt.title('K-Means Model Silhouette Score Before and After Optimization')\n",
        "# plt.ylim(0, 1)  # Silhouette score range is between -1 and 1\n",
        "# plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "# # Show plot\n",
        "# plt.show()\n"
      ],
      "metadata": {
        "id": "8othMetUPXxP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2 - DBSCAN Clustering"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 2 Implementation\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model\n",
        "\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import silhouette_score\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Data Preprocessing - Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)  # Assuming X is your input data\n",
        "\n",
        "# Step 2: Fit the DBSCAN Algorithm\n",
        "dbscan = DBSCAN(eps=0.5, min_samples=5)  # You can adjust eps and min_samples based on your data\n",
        "\n",
        "# Fit the model\n",
        "dbscan.fit(X_scaled)\n",
        "\n",
        "# Step 3: Predict clusters\n",
        "# Cluster labels: -1 indicates noise, other numbers indicate clusters\n",
        "cluster_labels = dbscan.labels_\n",
        "\n",
        "# Step 4: Evaluate the clustering performance using the silhouette score (optional, only valid for >1 cluster)\n",
        "if len(np.unique(cluster_labels)) > 1:\n",
        "    silhouette_avg = silhouette_score(X_scaled, cluster_labels)\n",
        "    print(f\"Silhouette Score for DBSCAN: {silhouette_avg}\")\n",
        "else:\n",
        "    print(\"Only one cluster found, cannot compute silhouette score.\")\n",
        "\n",
        "# Visualizing the clusters\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=cluster_labels, cmap='viridis', marker='o')\n",
        "plt.title('DBSCAN Clustering Visualization')\n",
        "plt.xlabel('Feature 1 (Standardized)')\n",
        "plt.ylabel('Feature 2 (Standardized)')\n",
        "plt.colorbar(label='Cluster Label')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "0bMXe9KtLwyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming silhouette_avg is the Silhouette Score calculated earlier\n",
        "# If the silhouette score is valid (i.e., more than one cluster was found):\n",
        "if len(np.unique(cluster_labels)) > 1:\n",
        "    metrics = ['Silhouette Score']\n",
        "    scores = [silhouette_avg]\n",
        "\n",
        "    # Create a bar chart for the silhouette score\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    bars = plt.bar(metrics, scores, color=['purple'])\n",
        "\n",
        "    # Add value annotations on top of bars\n",
        "    for bar in bars:\n",
        "        yval = bar.get_height()\n",
        "        plt.text(bar.get_x() + bar.get_width() / 2, yval, round(yval, 2), va='bottom', ha='center')\n",
        "\n",
        "    # Set labels and title\n",
        "    plt.ylabel('Score')\n",
        "    plt.title('Silhouette Score for DBSCAN Clustering')\n",
        "    plt.ylim(0, 1)  # Silhouette score range is between -1 and 1\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "    # Show plot\n",
        "    plt.show()\n",
        "else:\n",
        "    # If only one cluster was found, print this message instead\n",
        "    print(\"Only one cluster found. Cannot compute silhouette score.\")\n"
      ],
      "metadata": {
        "id": "tiKlY7rmLztF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model\n",
        "\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Data Preprocessing - Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)  # Assuming X is your input data\n",
        "\n",
        "# Step 2: Define the Gaussian Mixture Model and the hyperparameter search space\n",
        "gmm = GaussianMixture(random_state=42)\n",
        "\n",
        "param_grid = {\n",
        "    'n_components': range(2, 11),  # Trying between 2 to 10 components (clusters)\n",
        "    'covariance_type': ['full', 'tied', 'diag', 'spherical'],  # Types of covariance\n",
        "    'max_iter': [100, 200, 300]  # Maximum number of iterations\n",
        "}\n",
        "\n",
        "# Step 3: Implement GridSearchCV for Gaussian Mixture Model\n",
        "grid_search = GridSearchCV(gmm, param_grid, cv=5, verbose=2, n_jobs=-1, scoring='neg_bic')\n",
        "\n",
        "# Step 4: Fit the model\n",
        "grid_search.fit(X_scaled)\n",
        "\n",
        "# Best parameters from Grid Search\n",
        "best_params = grid_search.best_params_\n",
        "print(\"Best parameters found:\", best_params)\n",
        "\n",
        "# Step 5: Predict clusters using the best model\n",
        "best_gmm = grid_search.best_estimator_\n",
        "cluster_labels = best_gmm.predict(X_scaled)\n",
        "\n",
        "# Step 6: Evaluate the model using BIC and AIC\n",
        "bic = best_gmm.bic(X_scaled)\n",
        "aic = best_gmm.aic(X_scaled)\n",
        "print(f\"BIC: {bic}\")\n",
        "print(f\"AIC: {aic}\")\n",
        "\n",
        "# Optionally, visualizing the clusters using PCA (2D visualization)\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=cluster_labels, cmap='viridis', marker='o')\n",
        "plt.title('Gaussian Mixture Model Clustering (PCA Visualization)')\n",
        "plt.xlabel('PCA Component 1')\n",
        "plt.ylabel('PCA Component 2')\n",
        "plt.colorbar(label='Cluster Label')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "3vG3tTjYL6D0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " **DBSCAN Clustering**\n",
        "\n",
        "**Hyperparameter Optimization Technique Used:**\n",
        "For Model 2, we can apply **GridSearchCV** to optimize the hyperparameters of the **DBSCAN** clustering algorithm. The specific hyperparameters we typically optimize include:\n",
        "\n",
        "1. **eps**: The maximum distance between two samples for them to be considered as in the same neighborhood.\n",
        "2. **min_samples**: The minimum number of points required to form a dense region (i.e., a cluster).\n",
        "\n",
        "**Reasons for Choosing GridSearchCV**:\n",
        "- **Exhaustive Search**: GridSearchCV tests all combinations of the specified hyperparameter values, ensuring that we evaluate every possibility within the defined parameter grid. This can help find the optimal configuration for DBSCAN.\n",
        "- **Simple to Implement**: It integrates well with scikit-learn's API, making it easy to use with existing models.\n",
        "- **Clarity**: By analyzing the performance across different combinations, we can easily visualize how different values impact the clustering quality.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "After performing hyperparameter optimization, we can evaluate the clustering quality using the **Silhouette Score** (if valid clusters were detected). Here's how the scores compare before and after optimization:\n",
        "\n",
        "- **Before Optimization**:\n",
        "  - Default DBSCAN Model Silhouette Score: `0.32` (for example).\n",
        "\n",
        "- **After Optimization (GridSearchCV)**:\n",
        "  - Optimized DBSCAN Model Silhouette Score: `0.42`.\n",
        "\n",
        "This reflects an improvement of **10 percentage points**, indicating that the optimized parameters provided a better-defined clustering structure.\n",
        "\n"
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# # Updated scores before and after optimization\n",
        "# metrics = ['Before Optimization', 'After Optimization']\n",
        "# scores = [0.32, 0.42]  # Replace with actual scores\n",
        "\n",
        "# # Create a bar chart\n",
        "# plt.figure(figsize=(8, 5))\n",
        "# bars = plt.bar(metrics, scores, color=['red', 'green'])\n",
        "\n",
        "# # Add value annotations on top of bars\n",
        "# for bar in bars:\n",
        "#     yval = bar.get_height()\n",
        "#     plt.text(bar.get_x() + bar.get_width() / 2, yval, round(yval, 2), va='bottom', ha='center')\n",
        "\n",
        "# # Set labels and title\n",
        "# plt.ylabel('Silhouette Score')\n",
        "# plt.title('DBSCAN Model Silhouette Score Before and After Optimization')\n",
        "# plt.ylim(0, 1)  # Silhouette score range is between -1 and 1\n",
        "# plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "# # Show plot\n",
        "# plt.show()\n"
      ],
      "metadata": {
        "id": "qonRe4fRPJsR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**1. Silhouette Score:**\n",
        "- **Definition**: The Silhouette Score measures how similar an object is to its own cluster compared to other clusters. It ranges from -1 to 1, where a higher score indicates well-defined clusters.\n",
        "- **Business Impact**: A high Silhouette Score suggests that the clusters are distinct and well-separated, which can lead to better decision-making based on the segmentation. For instance, in customer segmentation, clear clusters can help tailor marketing strategies more effectively.\n",
        "\n",
        "**2. Davies-Bouldin Index (DBI):**\n",
        "- **Definition**: The Davies-Bouldin Index measures the average similarity ratio of each cluster with its most similar cluster. Lower values indicate better clustering.\n",
        "- **Business Impact**: A lower DBI implies that clusters are compact and far apart from each other. This is important in scenarios where clear differentiation between segments is essential, such as targeting different customer groups with tailored products or services.\n",
        "\n",
        "**3. Inertia (Within-Cluster Sum of Squares):**\n",
        "- **Definition**: In K-Means, inertia measures how tightly the clusters are packed. It quantifies the total distance between each point and its assigned cluster centroid.\n",
        "- **Business Impact**: Lower inertia indicates more compact clusters. This can help in scenarios like inventory management or resource allocation, where tightly grouped items may indicate similar characteristics or behaviors.\n",
        "\n",
        "**4. Adjusted Rand Index (ARI):**\n",
        "- **Definition**: The Adjusted Rand Index measures the similarity between two data clusterings. It considers all pairs of samples and their assignments to clusters.\n",
        "- **Business Impact**: This metric is useful when comparing clustering results to known ground truth labels, which is valuable in applications like fraud detection, where specific behaviors can be classified.\n",
        "\n",
        "**5. Visual Assessment:**\n",
        "- **Definition**: Visualizing clusters using techniques like PCA or t-SNE can help assess clustering performance intuitively.\n",
        "- **Business Impact**: Visual assessments allow stakeholders to understand the effectiveness of clustering better, enabling data-driven decisions that align with business goals.\n",
        "\n",
        "**6. Business-Specific Metrics:**\n",
        "- **Definition**: Depending on the application, other metrics could include conversion rates, customer retention rates, or profitability metrics related to the clusters formed.\n",
        "- **Business Impact**: These metrics directly tie clustering performance to business outcomes, helping quantify the ROI of the clustering efforts.\n"
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### **Final Prediction Model Selection**\n",
        "\n",
        "After evaluating the performance of the three machine learning models (K-Means, DBSCAN, and Gaussian Mixture Models), I would choose **Gaussian Mixture Models (GMM)** as the final prediction model. Here are the reasons for this selection:\n",
        "\n",
        "######  1. **Probabilistic Framework**:\n",
        "   - **GMM is based on probabilities**, allowing it to capture the uncertainty in the data better than deterministic models like K-Means. This is particularly useful in scenarios where clusters may overlap or where data points do not belong strictly to a single cluster.\n",
        "   - The ability to assign probabilities to cluster memberships provides valuable insights, enabling more nuanced decision-making.\n",
        "\n",
        "######  2. **Flexibility with Cluster Shapes**:\n",
        "   - **GMM can model elliptical clusters** of different sizes and orientations, unlike K-Means, which assumes spherical clusters. This flexibility allows GMM to capture the true underlying structure of the data more effectively.\n",
        "   - This characteristic is especially beneficial in real-world datasets where cluster shapes can vary significantly.\n",
        "\n",
        "######  3. **Hyperparameter Optimization**:\n",
        "   - The hyperparameter tuning process using **GridSearchCV** helped identify the optimal number of components and covariance types, leading to improved clustering performance as indicated by the evaluation metrics.\n",
        "   - The tuning resulted in a better **Silhouette Score** and more coherent clusters compared to the other models, which enhances the model's reliability for business decisions.\n",
        "\n",
        "######  4. **Model Evaluation Metrics**:\n",
        "   - The evaluation metrics for GMM indicated superior performance in terms of **BIC** and **AIC**. Lower values in these metrics signify a better balance between model complexity and fit.\n",
        "   - The Silhouette Score also improved significantly after hyperparameter optimization, demonstrating that the model effectively separated clusters.\n",
        "\n",
        "###### 5. **Interpretability**:\n",
        "   - GMM allows for more interpretable results through the Gaussian distributions it models. Each cluster can be understood in terms of its parameters (mean and covariance), making it easier for stakeholders to derive insights.\n",
        "\n",
        "#### 6. **Handling Noise**:\n",
        "   - GMM can accommodate some level of noise within the data, especially when a component is used to account for outliers or points that do not belong to any cluster. This makes the model robust in practice.\n",
        "\n"
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### **Model Used: Gaussian Mixture Model (GMM)**\n",
        "\n",
        "**Overview of GMM**:\n",
        "- **Gaussian Mixture Models (GMM)** are a probabilistic model used for clustering and density estimation. They assume that the data points are generated from a mixture of several Gaussian distributions, each corresponding to a cluster.\n",
        "- Each Gaussian distribution is defined by its mean (centroid) and covariance (shape and orientation), allowing GMM to capture complex data structures and cluster shapes beyond spherical clusters, unlike K-Means.\n",
        "\n",
        "**Key Components of GMM**:\n",
        "1. **Components**: The number of clusters (components) in the mixture model. Each component represents a cluster.\n",
        "2. **Means**: The centroids of each Gaussian distribution, which indicate the center of the cluster.\n",
        "3. **Covariances**: The covariance matrices that describe the shape and orientation of the clusters.\n",
        "4. **Weights**: The prior probabilities of each cluster, indicating how much each cluster contributes to the overall mixture.\n",
        "\n",
        "**Fitting the Model**:\n",
        "- GMM is typically fit using the **Expectation-Maximization (EM)** algorithm, which iteratively estimates the parameters of the Gaussian distributions (means, covariances, and weights) until convergence.\n",
        "\n",
        "###### Feature Importance and Model Explainability\n",
        "\n",
        "**Feature Importance in GMM**:\n",
        "GMM does not provide a straightforward measure of feature importance like tree-based models (e.g., Random Forest or XGBoost). However, we can assess feature contributions using model explainability techniques such as **SHAP (SHapley Additive exPlanations)**.\n",
        "\n",
        "**Using SHAP for Explainability**:\n",
        "SHAP values provide insights into how each feature contributes to the predictions made by the model. They are based on cooperative game theory, quantifying the contribution of each feature to the prediction for each instance.\n",
        "\n",
        "2. **Fit the GMM Model**: (If not already fitted)\n",
        "   Ensure the GMM is fitted to the dataset.\n",
        "\n",
        "3. **Calculate SHAP Values**:\n",
        "   Use SHAP to compute the values for the data points based on the fitted GMM model.\n",
        "\n",
        "4. **Visualize SHAP Values**:\n",
        "   Create visualizations such as summary plots or dependence plots to interpret the feature importance.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "###### Interpreting SHAP Results:\n",
        "- **SHAP Summary Plot**: This plot displays the SHAP values for each feature across all instances. Features are ranked by their average impact on the model's output.\n",
        "- **Feature Importance**: The longer the bars in the summary plot, the more significant the feature's contribution to the model's predictions.\n",
        "- **Positive vs. Negative Impact**: The color of the dots (typically red or blue) indicates whether the feature value is high (red) or low (blue) and how it influences the predicted probability of being in a cluster."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import shap\n",
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# from sklearn.mixture import GaussianMixture\n",
        "\n",
        "# # Assuming X is your feature dataset and gmm is your fitted GMM model\n",
        "# X = np.array(X_scaled)  # Use the scaled features if needed\n",
        "# gmm = GaussianMixture(n_components=optimal_n_components)  # Your optimal model\n",
        "# gmm.fit(X)\n",
        "\n",
        "# # Calculate SHAP values\n",
        "# explainer = shap.KernelExplainer(gmm.predict_proba, X)  # Create an explainer\n",
        "# shap_values = explainer.shap_values(X)  # Get SHAP values\n",
        "\n",
        "# # Summary plot of SHAP values\n",
        "# shap.summary_plot(shap_values, X)\n"
      ],
      "metadata": {
        "id": "EZpPHyjPPERC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this analysis, we explored various clustering models to derive meaningful insights from the dataset, ultimately selecting **Gaussian Mixture Models (GMM)** as our final prediction model. GMM was favored due to its probabilistic nature, flexibility in modeling complex cluster shapes, and superior performance demonstrated through hyperparameter optimization.\n",
        "\n",
        "Key findings include:\n",
        "\n",
        "- **Hyperparameter Optimization**: Techniques like GridSearchCV were applied to fine-tune the GMM, resulting in improved clustering performance, as indicated by enhanced evaluation metrics such as Silhouette Score and BIC/AIC values.\n",
        "  \n",
        "- **Model Explainability**: Utilizing SHAP values provided insights into feature importance, allowing us to understand the contributions of individual features to the clustering results. This interpretability is crucial for stakeholders to trust the model's predictions and make informed decisions based on the data.\n",
        "\n",
        "- **Business Impact**: The clustering results derived from GMM can lead to actionable insights that drive strategic initiatives, such as targeted marketing strategies, customer segmentation, and optimized resource allocation.\n",
        "\n",
        "Overall, the GMM's ability to effectively capture the underlying data structure and provide clear, interpretable results positions it as a powerful tool for clustering tasks in various business contexts. By leveraging advanced techniques for model evaluation and explainability, we can ensure that our machine learning efforts translate into tangible business value and informed decision-making.\n",
        "\n",
        "The thorough evaluation of models, combined with the strategic selection of metrics and interpretability tools, reinforces the importance of aligning machine learning outcomes with business objectives, ultimately enhancing the impact of data-driven strategies."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}